\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}


\title{GPU Virtualization in the Cloud (a mejorar el t\'itulo!!!)}
\author{\authorname{Sergio Iserte, Francisco J. Clemente-Castell\'o, Adri\'an Castell\'o,\\Rafael Mayo and Enrique S. Quintana-Ort\'i}
\affiliation{Department of Computer Science and Engineering\\Universitat Jaume I - Castell\'o de la Plana, Spain}
\email{\{siserte, fclement, adcastel, mayo, quintana\}@uji.es}}

\keywords{Cloud Computing, GPU Virtualization, Resource Management}

\abstract{The popularity of the GPGPU computation has led several cloud vendors to provide instances of virtual machines with GPUs. 
So that, the guest hosts must be equipped with GPUs which could be barely utilized if no GPU-enabled VM  is running in the host.
The solution presented here is based on GPU virtualization and shareability in order to reach an equilibrium between supply and demand of accelerators from the users. 
Hence, we propose to access any GPU from any VM getting rid of the necessity of having physical GPUs in the guest host. 
Finally, our project goes further and it takes into account that by making logical partitions of the GPU memory we could address each partition as if it was an independent GPU device.}

\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

\noindent 
Nowadays, to provide a virtual machine with GPUs means
that the guest host must be equipped with at least 1 GPU, and
once it has been assigned to an instance, no other VM can use
it. Depending on the users’ necessities, two approaches to introduce GPGPU computation in the cluster can be determined:
the conservative, to have a lot of hosts with GPUs to be
ensured that you are likely to satisfy all the requests; or a
more daring approach, where you will count with some GPU-
enabled nodes hoping not to arrive a large burst of requests
so the cluster would run out of GPU resources. Our solutions
is based on GPU virtualization and sharing resources in order
to reach a fair balance between supply and demand.
Several attempts of achieving what we are pursing have been
made, but none of them are so ambitious as our project. On
the one hand the work done in [1] allows the VM managed
by the hypervisor Xen to access the GPUs in the physical
node. With the implied limitations of restricting the nodes not
to use more GPUs than the hosted in the machine and not
to share idle GPUs to other machines. The solution presented
by the project gVirtus [2] does virtualize GPUs and make
them accessible for any VM in the cluster. However, this kind
of virtualization strongly depends on the hypervisor used, so
does its performance. Another similar solution is presented in
[3] by the name of gCloud. This one continues without being
integrated in a Cloud Computing Manager, though its main
drawback is that the code of the applications must be modified
in order to be run in their virtual-GPU environment. On the
other hand, the work exposed in [4] is more mature, however,
it is only focussed on compute intensive HPC applications.
The main idea of our proposal goes further and apart from
bringing solutions for all kind of HPC applications, it is aimed
to boost the cluster flexibility in the use of GPUs.

\section{\uppercase{Background}}
\label{sec:background}
\subsection{The rCUDA Framework}
\label{sec:rcuda}

{rCUDA}~\cite{tonithesis,toniparco} is a middleware that enables transparent access
to any NVIDIA GPU device present in a cluster from all compute
nodes. The GPUs can be accessed and shared between nodes, and a single node can use all these graphic accelerators
as if they were local.
These features are focused in attaining higher accelerator utilization rates in the overall system while simultaneously reducing
resource, space, and energy~\cite{energy14} requirements.
rCUDA is structured following a client-server distributed
architecture: the client middleware is allocated in the same cluster node where the application demanding GPGPU
acceleration services is executed, providing a transparent replacement for the
native CUDA libraries. Furthermore, the server middleware is executed in the
cluster nodes from which the actual GPUs provide the requested GPGPU service.
To support a concurrent scenario where GPUs are shared between
processes\slash nodes, {rCUDA} manages separate device contexts for
each client application.

The {rCUDA} 5.0 client exposes the same interface as the regular NVIDIA
CUDA 6.5 release~\cite{cuda65}, including the runtime and driver
APIs as well as other commonly used libraries such as cuBLAS, cuFFT, cuSparse or cuRand.
Therefore, applications are not aware of the fact that they are being executed
on top of a virtualization layer.
With the aim to be updated with new GPU programming models, {rCUDA} also supports
directive-based models such as OmpSS~\cite{repara15} and OpenACC~\cite{cluster15}.

The integration of remote GPGPU virtualization with global
resource schedulers such as SLURM~\cite{sbacpad14} completes this powerful
technology, making accelerator-enabled clusters more flexible and
energy efficient.

\subsection{Amazon Web Services}
\label{sec:aws}
AWS~\cite{aws} is a public cloud computing provider
and it is composed by several cloud services such as 
 cloud-based computation, storage and other func-
tionality that enables organizations and/or individuals to deploy
services and applications on an on-demand basis. 
These services, replace IT infrastructure for most companies, and provide agility and instant elasticity matching 
perfectly with their softwares requirements.

From the point of view of HPC, AWS offers high performance facilities including 
instances equiped with GPUs and high performance network interconnection which 
are common resources used in this research field.

\subsection{OpenStack}
\label{sec:openstack}
\begin{itemize}
\item Descripcion basica del openstack (Sisco?)
\end{itemize}

\section{Motivation}
\label{sec:motivation}

\begin{itemize}
\item Actual estado de las tecnolog\'ias de cloud
\item Limitaciones que nos llevan a realizar este trabajo
\item Beneficios de la unión de ambas tecnolog\'ias
\end{itemize}


\section{Remote GPUs in AWS}
\label{sec:rgpuaws}

\begin{itemize}
\item Escenarios creados
\item Experimentos
\item An\'alisis basados en flexibilidad y coste econ\'omico
\end{itemize}

\section{GPGPU Service for OpenStack}
\subsection{Managing remote GPUs with OpenStack}
The main idea of this project is to reach a solution that drive us to an scenario pretty similar the one that could be seen in Figure~\ref{fig:external}, where a new shared service takes part into the OpenStack Architecure.
This new service is in charge of GPGPU computation, bringing more flexibility when it comes to GPUs.
As it is shown in Figure~\ref{fig:internal}, we have altered the original Dashboard with a new parser, 
which split the HTTP query in order to make use of both, the GPGPU API for everything related to GPUs, and the nova API for the rest of the computation. 
Thanks to this modular approach, the Compute Module has not been modified, which led into an easily portability to other Cloud Computing environments.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{images/os1.jpg}
  \caption{Openstack with our GPGPU Service}
  \label{fig:external}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{images/os2.png}
  \caption{Internal Communication among modules}
  \label{fig:internal}
\end{figure}

\subsection{Development}
We have developed an independent module that can be seen as a new shared service in the OpenStack Architecture (see Figure~\ref{fig1}).

Thus, we have provided OpenStack with a new parser, which splits the HTTP query in order to make use of both, the GPGPU API for everything related to GPUs, and the nova API for the rest of the computation. 
Thanks to this modular approach, the Nova Project has not been modified and the tool could be easily ported to other Cloud Computing solutions.

The new GPGPU Service has brought a new feature when it comes to GPUs, from now on, we can create ``GPU-pools''. A ``GPU-pool'' is a set of independent GPUs (logically disattached from the nodes) that can be assigned to one or more instances.

Furthermore, we have extended the user interface (Horizon Project), in order to deal with the new features.
First of all, the Instance Launch Panel has been provided with a new field, where the user can assign a existent GPU pool, create a new one or leave the instance without accelerators of this kind.
As soon as the option ``New GPU Pool'' has been chosen, will appear the proper fields for the pool configuration.
Finally, with the purpose of being aware of the devices status, a new panel has been added with all the information regarding the GPUs.

\subsection{Working Modes}
The developed module will allow the users to create any of the following scenarios (see Figure~\ref{fig2}). 
The users are given two configuration options to decide whether a physical GPU will be completely allocated for an instance (first column) or the instance will address a partition of the GPU as if it were a device (second column).
We refer to this as the ``mode'' and the possible values for this parameter are: ``exclusive'' or ``shared''. 
Let us suppose to have 4 GPU devices in the cluster (no matter where they were hosted, neither if they were in the same server). 
A well illustrated example of those scenarios can be seen in the first row of the  Figure~\ref{fig2}.
So, while in ``exclusive'' mode the instance is monopolising all the GPUs, in the ``shared'' mode the GPUs have been partitioned. 
As a result of halving the GPU memory of the accelerators, the instance will be able to work with up to 8 GPUs.

Moreover, the users are also responsible for deciding whether a GPU (or a pool) will be assigned to other instances. 
This behaviour is refereed as ``scope'' and determines that a group of instances is logically connected to a pool of GPUs.
Working with ``public scope'' (second row of Figure~\ref{fig2}) means that the GPUs of a pool can be used simultaneously by all the instances linked to it.
Again, the GPU pool can be made of ``exclusive'' GPUs or ``shared''.

\begin{figure}[!t]
  \centering
  \includegraphics[width=.5\textwidth]{images/workingmodes.jpg}
  \caption{Examples of working modes}
  \label{fig2}
\end{figure}

\subsection{User Interface}
Several modifications have been planned in the OpenStack Dashboard in order to deal with the new features.
First of all, the Instance Launch Panel has been provided with a new field, where the user can assign a existent GPU pool, create a new one or leave the instance without accelerators of this kind.
As soon as the option ``New GPU Pool'' has been chosen, will appear the proper fields for the pool configuration (see Subfigure~\ref{fig:ui-launch}).
Furthermore, a new panel with the existent GPUs has been added to the Interface (see Subfigure~\ref{fig:ui-rgpus}), where all the information of the GPUs is displayed.

\begin{figure}[!t]
  \centering
  \includegraphics[width=.9\linewidth]{images/UI-launch.jpg}
  \caption{Launching Instances and assigning GPUs}
  \label{fig:ui-launch}
\end{figure}
  
\begin{figure}[!t]
  \centering
  \includegraphics[width=.85\linewidth]{images/UI-rgpus.jpg}
  \caption{GPU Information Panel}
  \label{fig:ui-rgpus}
\end{figure}

\subsection{Experimental Results}

\section{Discussion}
\label{sec:discussion}
Volver a hacer un resumen

\section*{\uppercase{Acknowledgements}}
The researchers from the Universidad Jaume~I were supported by Universitat Jaume I research project (P11B2013-21), project
TIN2011-23283 and~FEDER.

\bibliographystyle{apalike}
{\small
\bibliography{paper}}


\end{document}

